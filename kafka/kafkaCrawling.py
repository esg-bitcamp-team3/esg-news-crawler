from kafka import KafkaProducer
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import chromedriver_autoinstaller
import json
import time
import urllib.parse

# Kafka producer ì„¤ì •
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')
)

# í¬ë¡¬ ë“œë¼ì´ë²„ ìë™ ì„¤ì¹˜
chromedriver_autoinstaller.install()

# í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_news(company_name):
    encoded_query = urllib.parse.quote(company_name)
    url = f"https://search.hani.co.kr/search/newslist?searchword={encoded_query}&startdate=2025.01.01&enddate=2025.12.31&sort=desc"

    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    driver = webdriver.Chrome(options=options)
    driver.get(url)
    time.sleep(3)

    articles = []

    items = driver.find_elements(By.CSS_SELECTOR, "dl.search")
    for item in items[:5]:  # ì• 5ê°œë§Œ ê°€ì ¸ì˜¤ê¸°
        try:
            title = item.find_element(By.CSS_SELECTOR, "dt > a").text
            content = item.find_element(By.CSS_SELECTOR, "dd").text
            date = item.find_element(By.CSS_SELECTOR, ".date").text
            articles.append({
                "company": company_name,
                "title": title,
                "content": content,
                "date": date
            })
        except:
            continue

    driver.quit()
    return articles

# ì˜ˆì‹œ: "ì‚¼ì„±ì „ì" ê¸°ì‚¬ í¬ë¡¤ë§ í›„ Kafkaë¡œ ì „ì†¡
company_name = "ì‚¼ì„±ì „ì"
news_list = crawl_news(company_name)

for article in news_list:
    producer.send("news-raw", article)
    print(f"ğŸ“¤ Kafkaë¡œ ì „ì†¡ë¨: {article['title']}")

producer.flush()
print("âœ” ëª¨ë“  ê¸°ì‚¬ ì „ì†¡ ì™„ë£Œ")
