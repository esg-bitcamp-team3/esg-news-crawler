from kafka import KafkaConsumer
import json
import re
import torch
import pandas as pd
from kobert_transformers import get_kobert_model, get_tokenizer
from transformers import BertTokenizer, BertForSequenceClassification
import os
import pandas as pd
from pymongo import MongoClient

# ì €ì¥ íŒŒì¼ ì´ë¦„
csv_file = "esg_predictions.csv"

# Atlas URI
# MONGO_URI = "mongodb+srv://jinpang97:MONGOsj!0122@cluster0.bxnwcsi.mongodb.net/CrawlData"
MONGO_URI = "mongodb+srv://jinpang97:MONGOsj!0122@cluster0.bxnwcsi.mongodb.net/?retryWrites=true&w=majority"

# MongoDB ì—°ê²° ì„¤ì •
client = MongoClient(MONGO_URI)
db = client["CrawlData"]  # ì›í•˜ëŠ” DB ì´ë¦„
collection = db["crawlingPreprocessing"]  # ì›í•˜ëŠ” collection ì´ë¦„

def save_result_to_mongo(result):
    try:
        collection.insert_one(result)
        print("ğŸŒ MongoDB ì €ì¥ ì™„ë£Œ!")
    except Exception as e:
        print("âŒ MongoDB ì €ì¥ ì‹¤íŒ¨:", e)


# íŒŒì¼ì´ ì—†ìœ¼ë©´ í—¤ë” í¬í•¨ ì €ì¥, ìˆìœ¼ë©´ append
def save_result_to_csv(result):
    df_row = pd.DataFrame([result])
    if not os.path.exists(csv_file):
        df_row.to_csv(csv_file, index=False, encoding='utf-8-sig')
    else:
        df_row.to_csv(csv_file, mode='a', header=False, index=False, encoding='utf-8-sig')

# ëª¨ë¸ ì¤€ë¹„
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
kobert_model = get_kobert_model().to(device)
kobert_tokenizer = get_tokenizer()

bert_model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment', num_labels=5)
bert_tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')
bert_model.to(device)

# í‚¤ì›Œë“œ ë¡œë”©
df_E = pd.read_csv("./ESG_keywords/E_Environment_Keywords_Unique.csv")
df_S = pd.read_csv("./ESG_keywords/S_Social_Keywords_Unique.csv")
df_G = pd.read_csv("./ESG_keywords/G_Governance_Keywords_Unique.csv")

env_keywords = df_E["Keyword"].tolist()
soc_keywords = df_S["Keyword"].tolist()
gov_keywords = df_G["Keyword"].tolist()

# ë¬¸ì¥ ë¶„ë¦¬
def split_into_sentences(text):
    sentences = re.split(r'(?<=[.!?])\s+', text)
    return [s.strip() for s in sentences if s.strip()]

# ê°ì„± ë¶„ì„
def classify_sentiment(text):
    inputs = bert_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=64)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    with torch.no_grad():
        logits = bert_model(**inputs).logits
        pred = torch.argmax(logits, dim=1).item()
    return pred

# í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
def count_keywords(content, keywords):
    no_space = content.replace(" ", "")
    return sum((kw in content or kw.replace(" ", "") in no_space) for kw in keywords)

# Kafka Consumer
consumer = KafkaConsumer(
    'news-raw',
    bootstrap_servers='localhost:9092',
    auto_offset_reset='latest',
    enable_auto_commit=True,
    group_id='esg-analyzer',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)

print("âœ… ESG ë¶„ì„ Consumer ì‹œì‘ë¨...\n")

for msg in consumer:
    news = msg.value
    title = news['title']
    content = news.get('content') or news.get('summary') or ''
    company = news.get('company', 'N/A')
    date = news.get('date', 'N/A')

    # í‚¤ì›Œë“œ ê°œìˆ˜ ê³„ì‚°
    e_count = count_keywords(content, env_keywords)
    s_count = count_keywords(content, soc_keywords)
    g_count = count_keywords(content, gov_keywords)

    # ê°ì„± ë¶„ì„
    sentences = split_into_sentences(content)
    selected = sentences[:2] + sentences[-2:] if len(sentences) >= 4 else sentences
    sentiment_scores = [classify_sentiment(s) for s in selected]
    sentiment_avg = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 2  # ì¤‘ë¦½ê°’

    # ê²°ê³¼ êµ¬ì„±
    result = {
        "company": company,
        "date": date,
        "title": title,
        "E_value": e_count,
        "S_value": s_count,
        "G_value": g_count,
        "content_sentiment": sentiment_avg
    }

    print("ğŸ“Š ë¶„ì„ ì™„ë£Œ:", result)
    save_result_to_csv(result)  # CSV ì €ì¥
    save_result_to_mongo(result) #atlas ì €ì¥
    print("ì €ì¥ ì™„ë£Œ: esg_predictions.csv")
    print("-" * 80)

    # TODO: MongoDBì— ì €ì¥í•˜ê±°ë‚˜ Kafkaë¡œ ë‹¤ì‹œ ì „ì†¡ ê°€ëŠ¥

